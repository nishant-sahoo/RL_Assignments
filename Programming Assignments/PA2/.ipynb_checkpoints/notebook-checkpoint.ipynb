{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1 Environments\n",
        "# In this programming task, you’ll utilize the following Gymnasium environments for training\n",
        "# and evaluating your policies:\n",
        "# • Acrobot-v1: The system consists of two links connected linearly to form a chain, with\n",
        "# one end of the chain fixed. The joint between the two links is actuated. The goal is to\n",
        "# apply torques on the actuated joint to swing the free end of the linear chain above a\n",
        "# given height while starting from the initial state of hanging downwards.\n",
        "# • CartPole-v1: A pole is attached by an un-actuated joint to a cart, which moves along\n",
        "# a frictionless track. The pendulum is placed upright on the cart and the goal is to\n",
        "# balance the pole by applying forces in the left and right direction on the cart.\n",
        "\n",
        "# 2 Algorithms\n",
        "# You are tasked with training two variants of each Dueling-DQN and Monte-Carlo REINFORCE and assessing their comparative performance.\n",
        "\n",
        "# 2.1 Dueling-DQN\n",
        "# Dueling DQN is an extension of the DQN algorithm, designed to improve learning efficiency\n",
        "# by decomposing the Q-value function into two separate streams: one estimating the state\n",
        "# value and the other estimating the advantage of each action. The update equation for the\n",
        "# dueling network is:\n",
        "# Q(s, a; θ) = V (s; θ) + (A(s, a; θ) − 1/|A| sum_a′∈|A| A(s, a′; θ)) -> (Type-1)\n",
        "# Where Q(s, a; θ) represents the dueling Q-function with parameters θ.\n",
        "# Following is another way to estimate the Q-values:\n",
        "# Q(s, a; θ) = V (s; θ) +  A(s, a; θ) − max_{a′∈|A|} A(s, a′; θ) -> (Type-2)\n",
        "\n",
        "# Implement both update rules (Type-1) & (Type-2) and compare their performance in both the environments.\n",
        "\n",
        "\n",
        "\n",
        "# 2.2 Monte-Carlo REINFORCE\n",
        "# The MC-REINFORCE (Chapter 13) algorithm utilizes Monte Carlo sampling to estimate\n",
        "# gradients for policy optimization. The update equation of its policy parameter θ is given by\n",
        "# θ = θ + αGt ∇π(At |St , θ) / π(At |St , θ) -> (w/o Baseline)\n",
        "# In the presence of baseline, V (·; Φ) , the update equation is given by\n",
        "# θ = θ + α(Gt − V (St ; Φ)) ∇π(At |St , θ) / π(At |St , θ) -> (w/ Baseline)\n",
        "\n",
        "# The baseline V (·; Φ) is updated by TD(0) method.\n",
        "# Implement MC REINFORCE with both update methods ((w/o Baseline) &\n",
        "# (w/ Baseline)) and compare their performance in both the environments.\n",
        "\n",
        "# 3 Instructions\n",
        "# Four results plots (2 environments × 2 algorithms) (Ex. Plot 1 should compare (Type-1)\n",
        "# & (Type-2) Dueling DQN in Acrobot environment)\n",
        "# Use γ = 0.99 for all experiments\n",
        "# • Tune the hyper-parameter to minimize the regret in all experiments\n",
        "# • To account for stochasticity, use the average of 5 random seeds for each experiment/plot\n",
        "# • Plot the episodic return versus episodic number for every experiment\n",
        "# • The plots should consist the mean and variance across the 5 runs/seeds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViYREQU4eBXr"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import os\n",
        "import gym\n",
        "import csv\n",
        "# from gym.wrappers import Monitor\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils as nn_utils\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "import imageio\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiY9sXVneBXs"
      },
      "outputs": [],
      "source": [
        "def seed_all(seed=None):\n",
        "    if seed is None:\n",
        "        seed = 87 + 122\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEzPh6fNeBXt"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8992VzOfiID"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.h1_dim = 128\n",
        "        self.h2_dim = 128\n",
        "        self.gamma = 0.99 # Discount factor (constant)\n",
        "\n",
        "        self.lr = 1e-4 # Learning rate for policy network\n",
        "        self.batch_size = 32 # Batch size for experience replay\n",
        "        self.buffer_size = int(1e4) # Experience replay buffer size\n",
        "        self.target_update_freq = 20 # Frequency of updating target network\n",
        "\n",
        "        self.lr_theta = 1e-4 # Learning rate for policy network\n",
        "        self.lr_w = 1e-4 # Learning rate for value network\n",
        "\n",
        "        self.max_episodes = 1000\n",
        "        self.max_steps = 500\n",
        "        self.runs = 5\n",
        "        self.epsilon0 = 1.0\n",
        "        # self.min_epsilon = 0.001\n",
        "        # self.epsilon_decay = 0.99\n",
        "        self.min_epsilon = 0.0001\n",
        "        self.epsilon_decay = 0.98\n",
        "        self.print_freq = 200\n",
        "\n",
        "args = Args()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ghrTd1meBXu"
      },
      "outputs": [],
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "seed_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6voNYxCeBXu"
      },
      "source": [
        "## Dueling DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkKv1-3aeBXv"
      },
      "outputs": [],
      "source": [
        "# Experience Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        samples = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*samples)\n",
        "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYj8fp8peBXv"
      },
      "outputs": [],
      "source": [
        "# Dueling DQN\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, update_type=1, args=args):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, args.h1_dim)\n",
        "        self.fc2 = nn.Linear(args.h1_dim, args.h2_dim)\n",
        "\n",
        "        self.value = nn.Linear(args.h2_dim, 1)\n",
        "        self.advantage = nn.Linear(args.h2_dim, action_dim)\n",
        "        self.update_type = update_type\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu', mode='fan_in')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        value = self.value(x)\n",
        "        advantage = self.advantage(x)\n",
        "\n",
        "        if self.update_type == 1:\n",
        "            q = value + (advantage - advantage.mean(dim=-1, keepdim=True))\n",
        "        elif self.update_type == 2:\n",
        "            q = value + (advantage - advantage.max(dim=-1, keepdim=True)[0])\n",
        "        else:\n",
        "            raise NotImplementedError(\"Update type not implemented\")\n",
        "\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiqHcmhKeBXw"
      },
      "outputs": [],
      "source": [
        "# Dueling DQN Agent\n",
        "class DuelingDQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, update_type=1, args=args):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.update_type = update_type\n",
        "        self.args = args\n",
        "        self.q_net = DuelingDQN(state_dim, action_dim, update_type, self.args).to(device)\n",
        "        self.target_net = copy.deepcopy(self.q_net)\n",
        "        self.target_net.eval()\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.args.lr)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.replay_buffer = ReplayBuffer(self.args.buffer_size)\n",
        "        self.steps = 0\n",
        "\n",
        "    def act(self, state, epsilon):\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(self.action_dim)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        q_values = self.q_net(state)\n",
        "        return q_values.argmax().item()\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
        "        if len(self.replay_buffer) < self.args.batch_size:\n",
        "            return 0\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.args.batch_size)\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.LongTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        next_states = torch.FloatTensor(next_states).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "\n",
        "        q_values = self.q_net(states)\n",
        "        next_q_values = self.target_net(next_states).detach()\n",
        "\n",
        "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        next_q_value = next_q_values.max(dim=-1)[0]\n",
        "        target = rewards + self.args.gamma * next_q_value * (1 - dones)\n",
        "\n",
        "        loss = self.loss_fn(q_value, target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.steps += 1\n",
        "        if self.steps % self.args.target_update_freq == 0:\n",
        "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.q_net.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.q_net.load_state_dict(torch.load(path))\n",
        "        self.target_net = copy.deepcopy(self.q_net)\n",
        "        self.target_net.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kmjVLvueBXw"
      },
      "source": [
        "## Monte-Carlo REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL2urJ6leBXx"
      },
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, args=args):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, args.h1_dim)\n",
        "        self.fc2 = nn.Linear(args.h1_dim, args.h2_dim)\n",
        "        self.fc3 = nn.Linear(args.h2_dim, action_dim)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu', mode='fan_in')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.softmax(x, dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHI7BgoueBXx"
      },
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, args=args):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, args.h1_dim)\n",
        "        self.fc2 = nn.Linear(args.h1_dim, args.h2_dim)\n",
        "        self.fc3 = nn.Linear(args.h2_dim, 1)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu', mode='fan_in')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAkk1Yg-eBXx"
      },
      "outputs": [],
      "source": [
        "class MonteCarloREINFORCEAgent:\n",
        "    def __init__(self, state_dim, action_dim, baseline=False, args=args):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.args = args\n",
        "        self.policy_net = PolicyNetwork(state_dim, action_dim, self.args).to(device)\n",
        "        self.value_net = ValueNetwork(state_dim, self.args).to(device)\n",
        "        self.optimizer_theta = optim.Adam(self.policy_net.parameters(), lr=self.args.lr_theta)\n",
        "        self.optimizer_w = optim.Adam(self.value_net.parameters(), lr=self.args.lr_w)\n",
        "        self.baseline = baseline\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.steps = 0\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        probs = self.policy_net(state)\n",
        "        action = torch.distributions.Categorical(probs).sample()\n",
        "        return action.item()\n",
        "\n",
        "    def update(self, trajectory):\n",
        "        states, actions, rewards = zip(*trajectory)\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.LongTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "\n",
        "        returns = self._compute_returns(rewards)\n",
        "        # returns = (returns - returns.mean()) / (returns.std() + 1e-8) if self.baseline else returns\n",
        "\n",
        "        if self.baseline:\n",
        "            values = self.value_net(states).squeeze()\n",
        "            loss_w = self.loss_fn(values, returns)\n",
        "            self.optimizer_w.zero_grad()\n",
        "            loss_w.backward()\n",
        "            self.optimizer_w.step()\n",
        "\n",
        "        log_probs = torch.log(self.policy_net(states))\n",
        "        log_probs_actions = log_probs[range(len(actions)), actions]\n",
        "        loss_theta = -torch.sum(log_probs_actions * returns)\n",
        "\n",
        "        self.optimizer_theta.zero_grad()\n",
        "        loss_theta.backward()\n",
        "        self.optimizer_theta.step()\n",
        "\n",
        "        self.steps += 1\n",
        "        return loss_theta.item()\n",
        "\n",
        "    def _compute_returns(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.args.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        return torch.FloatTensor(returns).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OzP-4zjeBXy"
      },
      "outputs": [],
      "source": [
        "# Training for Dueling DQN and REINFORCE\n",
        "class Trainer:\n",
        "    def __init__(self, env_name, agent_type, update_type=None, baseline=False, args=args, save_results=True):\n",
        "        self.env_name = env_name\n",
        "        self.agent_type = agent_type\n",
        "        self.update_type = update_type\n",
        "        self.baseline = baseline\n",
        "        self.env = gym.make(env_name)\n",
        "        # self.env = gym.make(env_name, render_mode='human')\n",
        "        # self.env = Monitor(self.env, f'{env_name}_{agent_type}_{update_type}', force=True)\n",
        "        self.state_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = self.env.action_space.n\n",
        "        self.args = args\n",
        "        self.save_results = save_results\n",
        "\n",
        "    def get_agent(self):\n",
        "        if self.agent_type == \"dueling_dqn\":\n",
        "            return DuelingDQNAgent(self.state_dim, self.action_dim, self.update_type, self.args)\n",
        "        elif self.agent_type == \"reinforce\":\n",
        "            return MonteCarloREINFORCEAgent(self.state_dim, self.action_dim, self.baseline, self.args)\n",
        "        else:\n",
        "            raise NotImplementedError(\"Agent not implemented\")\n",
        "\n",
        "    def train(self):\n",
        "        rewards = np.zeros((self.args.runs, self.args.max_episodes))\n",
        "        losses = np.zeros((self.args.runs, self.args.max_episodes))\n",
        "        returns = np.zeros((self.args.runs, self.args.max_episodes))\n",
        "        for run in tqdm(range(self.args.runs)):\n",
        "            seed_all(seed=run)\n",
        "            self.agent = self.get_agent()\n",
        "            rewards[run], losses[run], returns[run] = self.train_single_run()\n",
        "            clear_output(wait=True)\n",
        "        # self.rewards_mean = rewards.mean(axis=0)\n",
        "        self.rewards = rewards\n",
        "        self.losses = losses\n",
        "        self.returns = returns\n",
        "        if self.save_results:\n",
        "            self.save() # Save the results\n",
        "        return rewards, losses, returns\n",
        "\n",
        "    def train_single_run(self):\n",
        "        if self.agent_type == \"dueling_dqn\":\n",
        "            return self.train_DuelingDQN()\n",
        "        elif self.agent_type == \"reinforce\":\n",
        "            return self.train_MC_REINFORCE()\n",
        "        else:\n",
        "            raise NotImplementedError(\"Agent not implemented\")\n",
        "\n",
        "    def train_DuelingDQN(self):\n",
        "        rewards = []\n",
        "        losses = []\n",
        "        episodic_returns = []\n",
        "        episodic_return = 0\n",
        "        # frames = []\n",
        "        for episode in range(self.args.max_episodes):\n",
        "            state = self.env.reset()\n",
        "            episode_reward = 0\n",
        "            for step in range(self.args.max_steps):\n",
        "                epsilon = max(self.args.min_epsilon, self.args.epsilon0 * self.args.epsilon_decay**self.agent.steps)\n",
        "                action = self.agent.act(state, epsilon)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                loss = self.agent.update(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "                episodic_return = self.args.gamma * episodic_return + reward\n",
        "                # frames.append(self.env.render())\n",
        "                if done:\n",
        "                    break\n",
        "            rewards.append(episode_reward)\n",
        "            losses.append(loss)\n",
        "            episodic_returns.append(episodic_return)\n",
        "            if episode % self.args.print_freq == 0:\n",
        "                print(\"Episode: {}, Reward: {}\".format(episode, episode_reward))\n",
        "        self.env.close()\n",
        "        # self.save_simulation(frames, f\"{self.env_name}_{self.agent_type}_{self.update_type}.mp4\")\n",
        "        return rewards, losses, episodic_returns\n",
        "\n",
        "    def train_MC_REINFORCE(self):\n",
        "        rewards = []\n",
        "        losses = []\n",
        "        episodic_returns = []\n",
        "        episodic_return = 0\n",
        "        for episode in range(self.args.max_episodes):\n",
        "            state = self.env.reset()\n",
        "            episode_reward = 0\n",
        "            trajectory = []\n",
        "            for step in range(self.args.max_steps):\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                trajectory.append((state, action, reward))\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "                episodic_return = self.args.gamma * episodic_return + reward\n",
        "                if done:\n",
        "                    break\n",
        "            rewards.append(episode_reward)\n",
        "            loss = self.agent.update(trajectory)\n",
        "            losses.append(loss)\n",
        "            episodic_returns.append(episodic_return)\n",
        "            if episode % self.args.print_freq == 0:\n",
        "                print(\"Episode: {}, Reward: {}\".format(episode, episode_reward))\n",
        "        return rewards, losses, episodic_returns\n",
        "\n",
        "    def save(self):\n",
        "        if self.agent_type == \"dueling_dqn\":\n",
        "            os.makedirs(\"results\", exist_ok=True)\n",
        "            np.save(f\"results/{self.env_name}_{self.agent_type}_{self.update_type}_rewards.npy\", self.rewards)\n",
        "            np.save(f\"results/{self.env_name}_{self.agent_type}_{self.update_type}_losses.npy\", self.losses)\n",
        "            np.save(f\"results/{self.env_name}_{self.agent_type}_{self.update_type}_returns.npy\", self.returns)\n",
        "        elif self.agent_type == \"reinforce\":\n",
        "            os.makedirs(\"results\", exist_ok=True)\n",
        "            np.save(f\"results/{self.env_name}_{self.agent_type}_{self.baseline}_rewards.npy\", self.rewards)\n",
        "            np.save(f\"results/{self.env_name}_{self.agent_type}_{self.baseline}_losses.npy\", self.losses)\n",
        "            np.save(f\"results/{self.env_name}_{self.agent_type}_{self.baseline}_returns.npy\", self.returns)\n",
        "        else:\n",
        "            raise NotImplementedError(\"Agent not implemented\")\n",
        "\n",
        "    @staticmethod\n",
        "    def save_simulation(frames, filename):\n",
        "        imageio.mimsave(filename, frames, fps=30)\n",
        "\n",
        "    # def save(self, path):\n",
        "    #     self.agent.save(path)\n",
        "\n",
        "    # def load(self, path):\n",
        "    #     self.agent.load(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def results_loader(env_name, agent_type, update_type=None, baseline=False):\n",
        "    if agent_type == \"dueling_dqn\":\n",
        "        rewards = np.load(f\"results/{env_name}_{agent_type}_{update_type}_rewards.npy\")\n",
        "        losses = np.load(f\"results/{env_name}_{agent_type}_{update_type}_losses.npy\")\n",
        "        returns = np.load(f\"results/{env_name}_{agent_type}_{update_type}_returns.npy\")\n",
        "    elif agent_type == \"reinforce\":\n",
        "        rewards = np.load(f\"results/{env_name}_{agent_type}_{baseline}_rewards.npy\")\n",
        "        losses = np.load(f\"results/{env_name}_{agent_type}_{baseline}_losses.npy\")\n",
        "        returns = np.load(f\"results/{env_name}_{agent_type}_{baseline}_returns.npy\")\n",
        "    else:\n",
        "        raise NotImplementedError(\"Agent not implemented\")\n",
        "    return rewards, losses, returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KEDc_zDeBXz"
      },
      "outputs": [],
      "source": [
        "def plot_rewards(rewards1, rewards2, title, save_path=None):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(rewards1.mean(axis=0), label=\"Type-1\")\n",
        "    plt.fill_between(np.arange(rewards1.shape[1]), rewards1.mean(axis=0) - rewards1.std(axis=0), rewards1.mean(axis=0) + rewards1.std(axis=0), alpha=0.3)\n",
        "    plt.plot(rewards2.mean(axis=0), label=\"Type-2\")\n",
        "    plt.fill_between(np.arange(rewards2.shape[1]), rewards2.mean(axis=0) - rewards2.std(axis=0), rewards2.mean(axis=0) + rewards2.std(axis=0), alpha=0.3)\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Episodic Rewards\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    if save_path:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(f\"{save_path}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_losses(losses1, losses2, title, save_path=None):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(losses1.mean(axis=0), label=\"Type-1\")\n",
        "    plt.fill_between(np.arange(losses1.shape[1]), losses1.mean(axis=0) - losses1.std(axis=0), losses1.mean(axis=0) + losses1.std(axis=0), alpha=0.3)\n",
        "    plt.plot(losses2.mean(axis=0), label=\"Type-2\")\n",
        "    plt.fill_between(np.arange(losses2.shape[1]), losses2.mean(axis=0) - losses2.std(axis=0), losses2.mean(axis=0) + losses2.std(axis=0), alpha=0.3)\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    if save_path:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(f\"{save_path}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_returns(returns1, returns2, title, save_path=None):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(returns1.mean(axis=0), label=\"Type-1\")\n",
        "    plt.fill_between(np.arange(returns1.shape[1]), returns1.mean(axis=0) - returns1.std(axis=0), returns1.mean(axis=0) + returns1.std(axis=0), alpha=0.3)\n",
        "    plt.plot(returns2.mean(axis=0), label=\"Type-2\")\n",
        "    plt.fill_between(np.arange(returns2.shape[1]), returns2.mean(axis=0) - returns2.std(axis=0), returns2.mean(axis=0) + returns2.std(axis=0), alpha=0.3)\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Episodic Returns\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    if save_path:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(f\"{save_path}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh6Vbxp7eBXz"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIuggWqTeBXz"
      },
      "outputs": [],
      "source": [
        "def dueling_dqn_acrobot_hyperparameter_tuning():\n",
        "    # Ensure the directory exists or create it if it doesn't\n",
        "    os.makedirs(\"hptuning/\", exist_ok=True)\n",
        "\n",
        "    tuned_args = Args()\n",
        "    batch_sizes = [32, 128, 1024]\n",
        "    buffer_sizes = [int(1e4), int(1e3)]\n",
        "    target_update_freqs = [20, 50]\n",
        "\n",
        "    best_hyperparameters = None\n",
        "    best_mean = -np.inf\n",
        "\n",
        "    # Define the filename for the CSV file\n",
        "    filename = 'hptuning/dueling_dqn_acrobot.csv'\n",
        "\n",
        "    # Write the data to a CSV file\n",
        "    with open(filename, 'w', newline='') as csvfile:\n",
        "        csvwriter = csv.writer(csvfile)\n",
        "        csvwriter.writerow(['Batch Size', 'Buffer Size', 'Target Update Freq', 'Average Rewards Mean'])\n",
        "\n",
        "        for batch_size in batch_sizes:\n",
        "            for buffer_size in buffer_sizes:\n",
        "                for target_update_freq in target_update_freqs:\n",
        "                    tuned_args.batch_size = batch_size\n",
        "                    tuned_args.buffer_size = buffer_size\n",
        "                    tuned_args.target_update_freq = target_update_freq\n",
        "                    trainer = Trainer(\"Acrobot-v1\", \"dueling_dqn\", update_type=1, args=tuned_args, save_results=False)\n",
        "                    rewards, _, _ = trainer.train()\n",
        "                    rewards_mean = rewards.mean(axis=0) # mean of rewards over 5 runs\n",
        "                    avg_rewards_mean = np.mean(rewards_mean) # mean of all episodes\n",
        "\n",
        "                    csvwriter.writerow([batch_size, buffer_size, target_update_freq, avg_rewards_mean])\n",
        "\n",
        "                    if avg_rewards_mean > best_mean:\n",
        "                        best_mean = avg_rewards_mean\n",
        "                        best_hyperparameters = (batch_size, buffer_size, target_update_freq)\n",
        "\n",
        "    tuned_args.batch_size, tuned_args.buffer_size, tuned_args.target_update_freq = best_hyperparameters\n",
        "    return tuned_args\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dueling_dqn_cartpole_hyperparameter_tuning():\n",
        "    # may tune on 100 episodes rather than 1000\n",
        "    os.makedirs(\"hptuning/\", exist_ok=True)\n",
        "    tuned_args = Args()\n",
        "    batch_sizes = [32, 128, 1024]\n",
        "    buffer_sizes = [int(1e4), int(1e3)]\n",
        "    target_update_freqs = [20, 50]\n",
        "\n",
        "    best_hyperparameters = None\n",
        "    best_mean = -np.inf\n",
        "\n",
        "    filename = 'hptuning/dueling_dqn_cartpole.csv'\n",
        "\n",
        "    with open(filename, 'w', newline='') as csvfile:\n",
        "        csvwriter = csv.writer(csvfile)\n",
        "        csvwriter.writerow(['Batch Size', 'Buffer Size', 'Target Update Freq', 'Average Rewards Mean'])\n",
        "\n",
        "        for batch_size in batch_sizes:\n",
        "            for buffer_size in buffer_sizes:\n",
        "                for target_update_freq in target_update_freqs:\n",
        "                    tuned_args.batch_size = batch_size\n",
        "                    tuned_args.buffer_size = buffer_size\n",
        "                    tuned_args.target_update_freq = target_update_freq\n",
        "                    trainer = Trainer(\"CartPole-v1\", \"dueling_dqn\", update_type=1, args=tuned_args, save_results=False)\n",
        "                    rewards, _, _ = trainer.train()\n",
        "                    rewards_mean = rewards.mean(axis=0) # mean of rewards over 5 runs\n",
        "                    avg_rewards_mean = np.mean(rewards_mean) # mean of all episodes\n",
        "\n",
        "                    csvwriter.writerow([batch_size, buffer_size, target_update_freq, avg_rewards_mean])\n",
        "\n",
        "                    if avg_rewards_mean > best_mean:\n",
        "                        best_mean = avg_rewards_mean\n",
        "                        best_hyperparameters = (batch_size, buffer_size, target_update_freq)\n",
        "\n",
        "\n",
        "    tuned_args.batch_size, tuned_args.buffer_size, tuned_args.target_update_freq = best_hyperparameters\n",
        "    return tuned_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mc_reinforce_acrobot_hyperparameter_tuning():\n",
        "    # may tune on 100 episodes rather than 1000\n",
        "    os.makedirs(\"hptuning/\", exist_ok=True)\n",
        "    tuned_args = Args()\n",
        "    h1_dims = [128, 256]\n",
        "    h2_dims = [64, 128]\n",
        "\n",
        "    best_hyperparameters = None\n",
        "    best_mean = -np.inf\n",
        "\n",
        "    # Define the filename for the CSV file\n",
        "    filename = 'hptuning/mc_reinforce_acrobot.csv'\n",
        "\n",
        "    # Write the data to a CSV file\n",
        "    with open(filename, 'w', newline='') as csvfile:\n",
        "        csvwriter = csv.writer(csvfile)\n",
        "        csvwriter.writerow(['Batch Size', 'Buffer Size', 'Target Update Freq', 'Average Rewards Mean'])\n",
        "\n",
        "        for h1_dim in h1_dims:\n",
        "            for h2_dim in h2_dims:\n",
        "                tuned_args.h1_dim = h1_dim\n",
        "                tuned_args.h2_dim = h2_dim\n",
        "                trainer = Trainer(\"Acrobot-v1\", \"reinforce\", baseline=False, args=tuned_args, save_results=False)\n",
        "                rewards, _, _ = trainer.train()\n",
        "                rewards_mean = rewards.mean(axis=0)\n",
        "                avg_rewards_mean = np.mean(rewards_mean)\n",
        "\n",
        "                csvwriter.writerow([h1_dim, h2_dim, avg_rewards_mean])\n",
        "\n",
        "                if avg_rewards_mean > best_mean:\n",
        "                    best_mean = avg_rewards_mean\n",
        "                    best_hyperparameters = (h1_dim, h2_dim)\n",
        "\n",
        "    tuned_args.h1_dim, tuned_args.h2_dim = best_hyperparameters\n",
        "    return tuned_args\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mc_reinforce_cartpole_hyperparameter_tuning():\n",
        "    # may tune on 100 episodes rather than 1000\n",
        "    os.makedirs(\"hptuning/\", exist_ok=True)\n",
        "    tuned_args = Args()\n",
        "    h1_dims = [128, 256]\n",
        "    h2_dims = [64, 128]\n",
        "\n",
        "    best_hyperparameters = None\n",
        "    best_mean = -np.inf\n",
        "\n",
        "    # Define the filename for the CSV file\n",
        "    filename = 'hptuning/mc_reinforce_cartpole.csv'\n",
        "\n",
        "    # Write the data to a CSV file\n",
        "    with open(filename, 'w', newline='') as csvfile:\n",
        "        csvwriter = csv.writer(csvfile)\n",
        "        csvwriter.writerow(['Batch Size', 'Buffer Size', 'Target Update Freq', 'Average Rewards Mean'])\n",
        "\n",
        "        for h1_dim in h1_dims:\n",
        "            for h2_dim in h2_dims:\n",
        "                tuned_args.h1_dim = h1_dim\n",
        "                tuned_args.h2_dim = h2_dim\n",
        "                trainer = Trainer(\"CartPole-v1\", \"reinforce\", baseline=False, args=tuned_args, save_results=False)\n",
        "                rewards, _, _ = trainer.train()\n",
        "                rewards_mean = rewards.mean(axis=0)\n",
        "                avg_rewards_mean = np.mean(rewards_mean)\n",
        "\n",
        "                csvwriter.writerow([h1_dim, h2_dim, avg_rewards_mean])\n",
        "\n",
        "                if avg_rewards_mean > best_mean:\n",
        "                    best_mean = avg_rewards_mean\n",
        "                    best_hyperparameters = (h1_dim, h2_dim)\n",
        "\n",
        "    tuned_args.h1_dim, tuned_args.h2_dim = best_hyperparameters\n",
        "    return tuned_args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQhcz6R5eBX0"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4ZMtwGrfiIM"
      },
      "source": [
        "### Dueling DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1hrybOSfiIM"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning\n",
        "args = Args()\n",
        "# args_ddqn_acrobot = dueling_dqn_acrobot_hyperparameter_tuning()\n",
        "# print(\"Tuned Hyperparameters Dueling DQN Acrobot: \", args_ddqn_acrobot.__dict__)\n",
        "# args = args_ddqn_acrobot\n",
        "\n",
        "# After tuning, the best hyperparameters for Dueling DQN Acrobot are:\n",
        "# 1024\t10000\t20\t-83.446\n",
        "args.batch_size = 1024\n",
        "args.buffer_size = int(1e4)\n",
        "args.target_update_freq = 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dueling DQN\n",
        "# Acrobot-v1 using Type-1 update\n",
        "env_name = \"Acrobot-v1\"\n",
        "agent_type = \"dueling_dqn\"\n",
        "trainer = Trainer(env_name, agent_type, update_type=1, args=args)\n",
        "rewards1, losses1, returns1 = trainer.train()\n",
        "\n",
        "# Acrobot-v1 using Type-2 update\n",
        "env_name = \"Acrobot-v1\"\n",
        "agent_type = \"dueling_dqn\"\n",
        "trainer = Trainer(env_name, agent_type, update_type=2, args=args)\n",
        "rewards2, losses2, returns2 = trainer.train()\n",
        "\n",
        "# Load results\n",
        "rewards1, losses1, returns1 = results_loader(\"Acrobot-v1\", \"dueling_dqn\", update_type=1)\n",
        "rewards2, losses2, returns2 = results_loader(\"Acrobot-v1\", \"dueling_dqn\", update_type=2)\n",
        "\n",
        "# Plots\n",
        "plot_rewards(rewards1, rewards2, \"Dueling DQN Acrobot-v1\", \"plots/dueling_dqn_acrobot_rewards\")\n",
        "plot_losses(losses1, losses2, \"Dueling DQN Acrobot-v1\", \"plots/dueling_dqn_acrobot_losses\")\n",
        "plot_returns(returns1, returns2, \"Dueling DQN Acrobot-v1\", \"plots/dueling_dqn_acrobot_returns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning\n",
        "args = Args()\n",
        "# args_ddqn_cartpole = dueling_dqn_cartpole_hyperparameter_tuning()\n",
        "# print(\"Tuned Hyperparameters Dueling DQN CartPole: \", args_ddqn_cartpole.__dict__)\n",
        "# args = args_ddqn_cartpole\n",
        "\n",
        "# 'batch_size': 128, 'buffer_size': 10000, 'target_update_freq': 20,\n",
        "args.batch_size = 128\n",
        "args.buffer_size = 10000\n",
        "args.target_update_freq = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iSInN3gheBX0",
        "outputId": "9701e39e-814b-43d5-b54b-670d3ee07049"
      },
      "outputs": [],
      "source": [
        "# Dueling DQN\n",
        "# CartPole-v1 using Type-1 update\n",
        "env_name = \"CartPole-v1\"\n",
        "agent_type = \"dueling_dqn\"\n",
        "trainer = Trainer(env_name, agent_type, update_type=1, args=args)\n",
        "rewards1, losses1, returns1 = trainer.train()\n",
        "\n",
        "# CartPole-v1 using Type-2 update\n",
        "env_name = \"CartPole-v1\"\n",
        "agent_type = \"dueling_dqn\"\n",
        "trainer = Trainer(env_name, agent_type, update_type=2, args=args)\n",
        "rewards2, losses2, returns2 = trainer.train()\n",
        "\n",
        "# Load results\n",
        "rewards1, losses1, returns1 = results_loader(\"CartPole-v1\", \"dueling_dqn\", update_type=1)\n",
        "rewards2, losses2, returns2 = results_loader(\"CartPole-v1\", \"dueling_dqn\", update_type=2)\n",
        "\n",
        "# Plots\n",
        "plot_rewards(rewards1, rewards2, \"Dueling DQN CartPole-v1\", \"plots/dueling_dqn_cartpole_rewards\")\n",
        "plot_losses(losses1, losses2, \"Dueling DQN CartPole-v1\", \"plots/dueling_dqn_cartpole_losses\")\n",
        "plot_returns(returns1, returns2, \"Dueling DQN CartPole-v1\", \"plots/dueling_dqn_cartpole_returns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTuHR1CafiIO"
      },
      "source": [
        "### Monte-Carlo REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nbI-t_afiIO"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning\n",
        "args = Args()\n",
        "args_mc_reinforce_acrobot = mc_reinforce_acrobot_hyperparameter_tuning()\n",
        "print(\"Tuned Hyperparameters MC REINFORCE Acrobot: \", args_mc_reinforce_acrobot.__dict__)\n",
        "args = args_mc_reinforce_acrobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UBYMv8dkeBX0",
        "outputId": "18e29f52-0bb5-45c2-d869-dbfa488f7f50"
      },
      "outputs": [],
      "source": [
        "# Monte Carlo REINFORCE\n",
        "# Acrobot-v1 without baseline\n",
        "env_name = \"Acrobot-v1\"\n",
        "agent_type = \"reinforce\"\n",
        "trainer = Trainer(env_name, agent_type, baseline=False, args=args)\n",
        "rewards1, losses1, returns1 = trainer.train()\n",
        "\n",
        "# Acrobot-v1 with baseline\n",
        "env_name = \"Acrobot-v1\"\n",
        "agent_type = \"reinforce\"\n",
        "trainer = Trainer(env_name, agent_type, baseline=True, args=args)\n",
        "rewards1, losses1, returns1 = trainer.train()\n",
        "\n",
        "# Load results\n",
        "rewards1, losses1, returns1 = results_loader(\"Acrobot-v1\", \"reinforce\", baseline=False)\n",
        "rewards2, losses2, returns2 = results_loader(\"Acrobot-v1\", \"reinforce\", baseline=True)\n",
        "\n",
        "# Plots\n",
        "plot_rewards(rewards1, rewards2, \"MC REINFORCE Acrobot-v1\", \"plots/mc_reinforce_acrobot_rewards\")\n",
        "plot_losses(losses1, losses2, \"MC REINFORCE Acrobot-v1\", \"plots/mc_reinforce_acrobot_losses\")\n",
        "plot_returns(returns1, returns2, \"MC REINFORCE Acrobot-v1\", \"plots/mc_reinforce_acrobot_returns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning\n",
        "args = Args()\n",
        "args_mc_reinforce_cartpole = mc_reinforce_cartpole_hyperparameter_tuning()\n",
        "print(\"Tuned Hyperparameters MC REINFORCE CartPole: \", args_mc_reinforce_cartpole.__dict__)\n",
        "args = args_mc_reinforce_cartpole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9oWZ7GP1eBX0",
        "outputId": "8d863201-5b92-4176-e465-db91382beb03"
      },
      "outputs": [],
      "source": [
        "# Monte Carlo REINFORCE\n",
        "# CartPole-v1 without baseline\n",
        "env_name = \"CartPole-v1\"\n",
        "agent_type = \"reinforce\"\n",
        "trainer = Trainer(env_name, agent_type, baseline=False, args=args)\n",
        "rewards1, losses1, returns1 = trainer.train()\n",
        "\n",
        "# CartPole-v1 with baseline\n",
        "env_name = \"CartPole-v1\"\n",
        "agent_type = \"reinforce\"\n",
        "trainer = Trainer(env_name, agent_type, baseline=True, args=args)\n",
        "rewards1, losses1, returns1 = trainer.train()\n",
        "\n",
        "# Load results\n",
        "rewards1, losses1, returns1 = results_loader(\"CartPole-v1\", \"reinforce\", baseline=False)\n",
        "rewards2, losses2, returns2 = results_loader(\"CartPole-v1\", \"reinforce\", baseline=True)\n",
        "\n",
        "# Plots\n",
        "plot_rewards(rewards1, rewards2, \"MC REINFORCE CartPole-v1\", \"plots/mc_reinforce_cartpole_rewards\")\n",
        "plot_losses(losses1, losses2, \"MC REINFORCE CartPole-v1\", \"plots/mc_reinforce_cartpole_losses\")\n",
        "plot_returns(returns1, returns2, \"MC REINFORCE CartPole-v1\", \"plots/mc_reinforce_cartpole_returns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-TS6-J5eBX1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "l6voNYxCeBXu",
        "eh6Vbxp7eBXz"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
