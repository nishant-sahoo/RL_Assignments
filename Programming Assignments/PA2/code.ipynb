{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "\n",
    "        # Advantage stream\n",
    "        self.advantage_fc = nn.Linear(64, output_size)\n",
    "\n",
    "        # Value stream\n",
    "        self.value_fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        advantage = self.advantage_fc(x)\n",
    "        value = self.value_fc(x)\n",
    "\n",
    "        # Combine value and advantage to get Q-values\n",
    "        q_values = value + (advantage - advantage.mean())\n",
    "        return q_values\n",
    "\n",
    "# Define replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, np.array([reward]), next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*samples)\n",
    "        return np.array(state_batch), action_batch, reward_batch, np.array(next_state_batch), done_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Define DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, capacity, batch_size, gamma, lr):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_size = action_size\n",
    "        self.buffer = ReplayBuffer(capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.q_network = DuelingDQN(state_size, action_size).to(self.device)\n",
    "        self.target_network = DuelingDQN(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.q_network(state)\n",
    "                return q_values.max(1)[1].item()\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        # Compute Q targets\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_network(next_states).max(1)[0]\n",
    "            q_targets = rewards + (1 - dones) * self.gamma * next_state_values\n",
    "\n",
    "        # Compute Q values\n",
    "        q_values = self.q_network(states)\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(q_values, q_targets)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# Main training loop\n",
    "def train_dueling_dqn(env, agent, num_episodes, max_steps, epsilon_start, epsilon_end, epsilon_decay):\n",
    "    epsilon = epsilon_start\n",
    "    rewards = []\n",
    "    losses = []\n",
    "    episode_states = []  # Collect states\n",
    "    episode_actions = []  # Collect actions\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_state = []  # Collect states for this episode\n",
    "        episode_action = []  # Collect actions for this episode\n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            agent.train()\n",
    "            episode_state.append(state)  # Collect state\n",
    "            episode_action.append(action)  # Collect action\n",
    "            if done:\n",
    "                break\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        rewards.append(total_reward)\n",
    "        if episode % 10 == 0:\n",
    "            agent.target_network.load_state_dict(agent.q_network.state_dict())\n",
    "        losses.append(agent.train())\n",
    "        episode_states.append(episode_state)\n",
    "        episode_actions.append(episode_action)\n",
    "        print(f\"Episode {episode}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "    # Plotting episode vs. reward\n",
    "    plt.plot(range(1, num_episodes + 1), rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Episode vs. Total Reward')\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting episode vs. loss\n",
    "    plt.plot(range(1, num_episodes + 1), losses)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Episode vs. Loss')\n",
    "    plt.show()\n",
    "\n",
    "    return episode_states, episode_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janmenjaya/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/janmenjaya/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/janmenjaya/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/tmp/ipykernel_80508/205527935.py:77: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  rewards = torch.FloatTensor(rewards).to(self.device)\n",
      "/home/janmenjaya/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64, 64])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, Total Reward: 24.0\n",
      "Episode 2/500, Total Reward: 15.0\n",
      "Episode 3/500, Total Reward: 11.0\n",
      "Episode 4/500, Total Reward: 17.0\n",
      "Episode 5/500, Total Reward: 11.0\n",
      "Episode 6/500, Total Reward: 14.0\n",
      "Episode 7/500, Total Reward: 36.0\n",
      "Episode 8/500, Total Reward: 16.0\n",
      "Episode 9/500, Total Reward: 24.0\n",
      "Episode 10/500, Total Reward: 16.0\n",
      "Episode 11/500, Total Reward: 21.0\n",
      "Episode 12/500, Total Reward: 15.0\n",
      "Episode 13/500, Total Reward: 25.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import gym\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    capacity = 10000\n",
    "    batch_size = 64\n",
    "    gamma = 0.99\n",
    "    lr = 1e-3\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size, capacity, batch_size, gamma, lr)\n",
    "    num_episodes = 500\n",
    "    max_steps = 1000\n",
    "    episode_states, episode_actions = train_dueling_dqn(env, agent, num_episodes, max_steps, epsilon_start, epsilon_end, epsilon_decay)\n",
    "\n",
    "    # Function to animate frames\n",
    "    def animate_frames(frames):\n",
    "        fig = plt.figure()\n",
    "        plt.axis('off')\n",
    "        ims = [[plt.imshow(frame, animated=True)] for frame in frames]\n",
    "        ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True)\n",
    "        ani.save('simulation.gif', writer='pillow', fps=30)\n",
    "\n",
    "    # Simulate agent's behavior and capture frames\n",
    "    frames = []\n",
    "    for episode_state, episode_action in zip(episode_states, episode_actions):\n",
    "        state = env.reset()\n",
    "        for action in episode_action:\n",
    "            frame = env.render(mode='rgb_array')\n",
    "            frames.append(frame)\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Call function to animate frames and save as GIF\n",
    "    animate_frames(frames)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
